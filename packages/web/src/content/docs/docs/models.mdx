---
title: Models
description: Configuring an LLM provider and model.
---

opencode uses the [AI SDK](https://ai-sdk.dev/) and [Models.dev](https://models.dev) to support for **75+ LLM providers** and it supports running local models.

---

## Providers

Most popular providers are preloaded by default. If you've added the credentials for a provider through `opencode auth login`, they'll be available when you start opencode.

Learn more about [providers](/docs/providers).

---

## Select a model

Once you've configured your provider you can select the model you want by typing in:

```bash frame="none"
/models
```

---

## Recommended models

There are a lot of models out there, with new models coming out every week.

:::tip
Consider using one of the models we recommend.
:::

However, there are a only a few of them that are good at both generating code and tool calling.

### Model Compatibility Chart

| Model               | Status                 | Intelligence | Cost       | Tool Calls | Context | Speed      | Best For                           |
| ------------------- | ---------------------- | ------------ | ---------- | ---------- | ------- | ---------- | ---------------------------------- |
| **Claude Sonnet 4** | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§    | ğŸ’°ğŸ’°ğŸ’°     | â­â­â­â­â­ | 200K    | âš¡âš¡âš¡âš¡   | Complex reasoning, large codebases |
| **Claude Opus 4**   | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§    | ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’° | â­â­â­â­â­ | 200K    | âš¡âš¡âš¡     | Most challenging tasks, research   |
| **GPT 4.1**         | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§ ğŸ§      | ğŸ’°ğŸ’°ğŸ’°     | â­â­â­â­   | 128K    | âš¡âš¡âš¡âš¡   | General development, debugging     |
| **Gemini 2.5 Pro**  | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§ ğŸ§      | ğŸ’°ğŸ’°       | â­â­â­â­   | 2M      | âš¡âš¡âš¡âš¡   | Large files, multimodal tasks      |
| **Kimi K2**         | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§        | ğŸ’°         | â­â­â­     | 200K    | âš¡âš¡âš¡âš¡âš¡ | Cost-effective, fast iteration     |
| **Qwen3 Coder**     | âœ… Fully Supported     | ğŸ§ ğŸ§ ğŸ§ ğŸ§      | ğŸ’°         | â­â­â­â­   | 32K     | âš¡âš¡âš¡âš¡âš¡ | Code generation, refactoring       |
| **GPT-4o**          | âš ï¸ Partially Supported | ğŸ§ ğŸ§ ğŸ§ ğŸ§      | ğŸ’°ğŸ’°       | â­â­â­     | 128K    | âš¡âš¡âš¡âš¡   | Multimodal, vision tasks           |
| **Claude Haiku**    | âš ï¸ Partially Supported | ğŸ§ ğŸ§          | ğŸ’°         | â­â­       | 200K    | âš¡âš¡âš¡âš¡âš¡ | Simple tasks, quick responses      |
| **Llama 3.3 70B**   | ğŸ§ª Experimental        | ğŸ§ ğŸ§ ğŸ§        | Free\*     | â­â­       | 128K    | âš¡âš¡âš¡     | Local deployment, privacy          |

**Legend:**

- **Status**: âœ… Fully Supported, âš ï¸ Partially Supported, ğŸ§ª Experimental
- **Intelligence**: ğŸ§  (1-5 scale, relative capability)
- **Cost**: ğŸ’° (1-5 scale, relative pricing per token)
- **Tool Calls**: â­ (1-5 scale, reliability rating)
- **Context**: Maximum context window size
- **Speed**: âš¡ (1-5 scale, response time)
- \*Free when self-hosted

### Known Limitations

- **GPT-4o**: Occasional tool call formatting issues with complex multi-step tasks
- **Claude Haiku**: Limited reasoning capability for complex architectural decisions
- **Llama models**: Tool calling reliability varies by hosting provider
- **Local models**: Performance depends heavily on hardware configuration

### Usage Recommendations

**For beginners**: Start with **Claude Sonnet 4** or **GPT 4.1** for the most reliable experience.

**For cost optimization**: **Kimi K2** or **Qwen3 Coder** offer excellent value for routine development tasks.

**For large codebases**: **Gemini 2.5 Pro** with its 2M context window can handle entire repositories.

**For privacy-sensitive work**: Consider **Llama 3.3 70B** with local deployment.

---

## Set a default

To set one of these as the default model, you can set the `model` key in your
opencode config.

```json title="opencode.json" {3}
{
  "$schema": "https://opencode.ai/config.json",
  "model": "lmstudio/google/gemma-3n-e4b"
}
```

Here the full ID is `provider_id/model_id`.

If you've configured a [custom provider](/docs/providers#custom), the `provider_id` is key from the `provider` part of your config, and the `model_id` is the key from `provider.models`.

---

## Loading models

When opencode starts up, it checks for the following:

1. The model list in the opencode config.

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "model": "anthropic/claude-sonnet-4-20250514"
   }
   ```

   The format here is `provider/model`.

2. The last used model.

3. The first model using an internal priority.
